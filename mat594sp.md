# MAT594SP: Aesthetics and Politics of Artificial Intelligence

Wednesdays and Fridays, 2-4, Elings 2003

## Description

This iteration of the MAT Artificial Intelligence Working Group starts from a basic hypothesis, put forward by Philip Agre in the late 1990s: ["AI is philosophy underneath"](https://web.stanford.edu/group/SHR/4-2/text/agre.html). Given the rapid development of the field since 2012, does this hypothesis hold? When we talk about artificial intelligence today, we talk about highly specialized machine learning models. Unlike in the 1990s, the primary function of these models is not the mechanization of reason but the mechanization of perception, most prominently the mechanization of vision. As a consequence, the tasks that many machine learning models operate on are aesthetic tasks, ranging from the classification of images in regard to their content and form to the generation of completely new images.</p>

At the same time, the technical opacity of many machine learning models makes it inherently difficult to properly evaluate their results. This is complicated even further whenever a model is deployed as a product and opacity becomes a desirable property. In fact, the interpretability of machine learning models — their ability to generate and/or facilitate explanations of their results — has not only become an independent field of research within computer science but has also grown into an increasingly important legal challenge. Hence, the once speculative phenomenological question "how does the machine perceive the world" suddenly becomes a real-world problem.

Contemporary machine learning models thus raise a set of issues that are completely independent of the ones raised by the possibility of a future general artificial intelligence. Most prominently, they are real-life socio-technical systems that have politics. Adapting Agre's hypothesis: AI is aesthetics and politics underneath.

Participants in the working group meet twice weekly to investigate this peculiar nexus of aesthetics and politics in contemporary machine learning through equal parts of critical reading and technical reviews (of technical papers and code examples).</p>

## Syllabus

:books: = article/book/blog post, :mortar_board: = talk, :computer: = source code close reading.</p>

### Wednesday, April 4, 2018: Introduction<

:books: Maciej Ceglowski, ["Superintelligence. The Idea That Eats Smart People"](http://idlewords.com/talks/superintelligence.htm) (2016)
:books: Victoria Krakovna, ["Is there a tradeoff between immediate and longer-term AI safety efforts?"](https://vkrakovna.wordpress.com/2018/01/27/is-there-a-tradeoff-between-safety-concerns-about-current-and-future-ai-systems/)
          </ul>

          <h3>Friday, April 6, 2018: Artificial Intelligence as a Philosophical Project</h3>
          <ul>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/zentralwerkstatt/AIWG/blob/master/1-philosophy.ipynb">zentralwerkstatt/AIWG: lecture notebook</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Alan M. Turing, <a href="https://www.jstor.org/stable/2251299">"Computing Machinery and Intelligence"</a>, Mind 59, no. 236 (1950), 433–60</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;John R. Searle, <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/DC644B47A4299C637C89772FACC2706A/S0140525X00005756a.pdf/minds_brains_and_programs.pdf">"Minds, Brains, and Programs,"</a> Behavioral and Brain Sciences 3, no. 3 (1980), 417–24</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Thomas Nagel, <a href="https://www.jstor.org/stable/2183914">"What It’s Like to Be a Bat"</a>, The Philosophical Review 83, no. 4 (1974), 435–450</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Philip E. Agre, <a href="https://web.stanford.edu/group/SHR/4-2/text/agre.html">"The Soul Gained and Lost. Artificial Intelligence as a Philosophical Project"</a>, SEHR 4, no. 2 (1995)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Joseph Weizenbaum, <a href="https://dl.acm.org/citation.cfm?id=365168">"ELIZA — a computer program for the study of natural language communication between man and machine"</a>, Communications of the ACM 9, no. 1 (1966), 36—45</li>
          </ul>

          <h3>Wednesday, April 11, 2018: History of Deep Learning</h3>
          <ul>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Fjodor van Veen, <a href="http://www.asimovinstitute.org/neural-network-zoo/">"The Neural Network Zoo"</a> (2016)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Andrey Kurenkov, <a href="http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/">"A 'Brief' History of Neural Nets and Deep Learning"</a> (2015)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">"Imagenet Classification with Deep Convolutional Neural Networks"</a> Advances in Neural Information Processing Systems (2012)</li>
          </ul>

          <h3>Friday, April 13, 2018: Limits of Deep Learning</h3>
          <ul>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/zentralwerkstatt/AIWG/blob/master/2-generalization.ipynb">zentralwerkstatt/AIWG: lecture notebook</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Yarden Katz, <a href="https://ssrn.com/abstract=3078224">"Manufacturing an Artificial Intelligence Revolution"</a> (2017)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Gary Marcus, <a href="https://arxiv.org/abs/1801.00631">"Deep Learning. A Critical Appraisal"</a> (2017), arXiv preprint 1801.0063</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J.Gershman, <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/A9535B1D745A0377E16C590E14B94993/S0140525X16001837a.pdf/div-class-title-building-machines-that-learn-and-think-like-people-div.pdf">"Building machines that learn and think like people"</a>, Behavioral and Brain Sciences 40 (2017)</li>
          </ul>

          <h3>Wednesday, April 18, 2018: Deep Dreaming I</h3>
          <ul>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/zentralwerkstatt/AIWG/blob/master/3-deepdream.ipynb">zentralwerkstatt/AIWG: lecture notebook</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Kyle McDonald, <a href="https://medium.com/@kcimc/a-return-to-machine-learning-2de3728558eb">"A Return to Machine Learning"</a> (2016), Medium</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Alexander Mordvintsev, Christopher Olah, Mike Tyka, <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">"Inceptionism: Going Deeper into Neural Networks"</a> (2015)</li>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/google/deepdream">google/deepdream</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Kyle Chayka, <a href="https://psmag.com/environment/googles-deep-dream-is-future-kitsch">"Why Google's Deep Dream is Future Kitsch"</a> (2015)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Alex Rayner, <a href="https://www.theguardian.com/artanddesign/2016/mar/28/google-deep-dream-art">"Can Google’s Deep Dream become an art machine?"</a> The Guardian (2016)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;<a href="https://grayarea.org/event/deepdream-the-art-of-neural-networks/">DeepDream: The art of neural networks</a> (exhibition, 2016)</li>
          </ul>

          <h3>Friday, April 20, 2018: Deep Dreaming II</h3>
          <ul>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/zentralwerkstatt/AIWG/blob/master/3-deepdream.ipynb">zentralwerkstatt/AIWG: lecture notebook</a></li>
          </ul>

          <h3>Wednesday, April 25, 2018: Feature Visualization I</h3>
          <ul>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/zentralwerkstatt/AIWG/blob/master/4-features.ipynb">zentralwerkstatt/AIWG: lecture notebook</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Francçois Chollet, <a href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html">"How Convolutional Neural Networks See the World"</a> (2016)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, Alexander Mordvintsev, <a href="https://distill.pub/2018/building-blocks/">"The Building Blocks of Interpretability"</a> (2018), Distill</li>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/tensorflow/lucid">tensorflow/lucid</a></li>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/zentralwerkstatt/AIWG/blob/master/4-features.ipynb">zentralwerkstatt/AIWG: feature visualization example notebook</a></li>
          </ul>

          <h3>Friday, April 27, 2018: Feature Visualization II</h3>
          <ul>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/zentralwerkstatt/AIWG/blob/master/4-features.ipynb">zentralwerkstatt/AIWG: lecture notebook</a></li>
            <li><span class="menu icon-university"></span>&nbsp;&nbsp;3pm: Hrushikesh Mhaskar, "A new look at machine learning as function approximation" (Webb Hall 1100)</li>
          </ul>
          
          <h3>Wednesday, May 2, 2018: Feature Visualization III</h3>
          <ul>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/zentralwerkstatt/AIWG/blob/master/5-nsfw.ipynb">zentralwerkstatt/AIWG: lecture notebook</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Gabriel Goh, <a href="https://open_nsfw.gitlab.io/">"Image Synthesis from Yahoo's open_nsfw"</a> (2016)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Fabian Offert, <a href="https://arxiv.org/abs/1711.08042">""I know it when I see it". Visualization and Intuitive Interpretability"</a> (2017), arXiv preprint 1711.08042</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus, <a href="https://arxiv.org/abs/1312.6199">"Intriguing properties of neural networks"</a> (2013), arXiv preprint 1312.6199</li>
          </ul>

          <h3>Friday, May 4, 2018: Interpretability I</h3>
          <ul>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Zachary C. Lipton, <a href="https://arxiv.org/abs/1606.03490">"The Mythos of Model Interpretability"</a> (2017), arXiv preprint 1606.03490</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Edsger W. Dijkstra, <a href="https://www.cs.utexas.edu/users/EWD/transcriptions/EWD09xx/EWD936.html">"On Anthropomorphism in Science"</a> (1985)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Been Kim, Finale Doshi-Velez, <a href="https://arxiv.org/abs/1702.08608">"Towards a rigorous science of interpretable machine learning"</a> (2017), arXiv preprint 1702.08608</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, Finale Doshi-Velez, <a href="https://arxiv.org/abs/1802.00682">"How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation"</a> (2018), arXiv preprint 1802.00682</li>
          </ul>

          <h3>Wednesday, May 9, 2018: GANs</h3>
          <ul>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/zentralwerkstatt/AIWG/blob/master/6-gans.ipynb">zentralwerkstatt/AIWG: lecture notebook</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">"Generative adversarial nets"</a>, Advances in Neural Information Processing Systems (2014), 2672–2680</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Chapter 4 only: Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, <a href="https://arxiv.org/abs/1606.03498">"Improved Techniques for Training GANs"</a> (2016), arXiv preprint 1606.03498</li>
          </ul>

          <h3>Friday, May 11, 2018</h3>
          <ul>
            <li>No class</li>
          </ul>

          <h3>Wednesday, May 16, 2018: Word Embeddings</h3>
          <ul>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/zentralwerkstatt/AIWG/blob/master/7-nlp.ipynb">zentralwerkstatt/AIWG: lecture notebook</a></li>
            <li><span class="menu icon-university"></span>&nbsp;&nbsp;12pm: William Wang, "Deep Learning for Computational Social Science" (SSMS 1310)</li>
            <li>Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, <a href="https://arxiv.org/abs/1301.3781">"Efficient Estimation of Word Representations in Vector Space"</a> (2013), arXiv Preprint arXiv:1301.3781</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Fabian Offert, <a href="post_vsm.html">Intuition and Epistemology of High-Dimensional Vector Space"</a> (2017)</li>
            <li><span class="icon-github"></span>&nbsp;&nbsp;&nbsp;<a href="http://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/">Rob Speer, "How to make a racist AI without really trying" (2017)</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam Kalai, <a href="https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf">"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings"</a>, Advances in Neural Information Processing Systems (2016)</li>
          </ul>

          <h3>Friday, May 18, 2018: Reinforcement Learning</h3>
          <ul>
            <li><span class="menu icon-university"></span>&nbsp;&nbsp;2pm: <a href="http://rodgerluo.com/">Rodger Luo</a> and Sam Green, "Visualization for Deep Reinforcement Learning" (Elings 2003)</li>
          </ul>

          <h3>Wednesday, May 23, 2018: RNNs</h3>
          <ul>
            <li><span class="menu icon-github"></span>&nbsp;&nbsp;<a href="https://github.com/zentralwerkstatt/AIWG/blob/master/7-nlp.ipynb">zentralwerkstatt/AIWG: lecture notebook</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Andrej Karpathy, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">"The Unreasonable Effectiveness of Recurrent Neural Networks"</a> (2015)</li>
            <li><span class="icon-github"></span>&nbsp;&nbsp;&nbsp;Yoav Goldberg, <a href="http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139">"The unreasonable effectiveness of Character-level Language Models"</a></li>
          </ul>

          <h3>Friday, May 25, 2018: FAT</h3>
          <ul>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;<a href="https://www.youtube.com/watch?v=fMym_BKWQzk&t=698s">Kate Crawford, "The Trouble with Bias", NIPS 2017 Keynote</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Heather Murphy, <a href="https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html">"Why Stanford Researchers Tried to Create a 'Gaydar' Machine"</a> (2017), New York Times</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Greggor Mattson, <a href="https://greggormattson.com/2017/09/09/artificial-intelligence-discovers-gayface/">"Artificial Intelligence Discovers Gayface. Sigh"</a> (2017)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Joy Buolamwini, Timnit Gebru, <a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"</a>, Proceedings of Machine Learning Research 81: Conference on Fairness, Accountability and Transparency, 23-24 February 2018, New York, NY, USA, 1–15</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Excerpts from: Frank Pasquale, The Black Box Society (Harvard University Press, 2015)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Zach Blas, <a href="http://www.zachblas.info/works/facial-weaponization-suite/">Facial Weaponization Suite</a> (2011-2014)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Mahmood Sharif, Sruti Bhagavatula, Michael K. Reiter, Lujo Bauer, <a href="https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf">"Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition"</a>, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 1528-1540</li>
          </ul>

          <h3>Wednesday, May 30, 2018: Interpretability II</h3>
          <ul>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Mike Ananny, Kate Crawford, <a href="http://journals.sagepub.com/doi/full/10.1177/1461444816676645">"Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability"</a> (2016), New Media and Society 1-17</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Andrew D. Selbst, Solon Barocas, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3126971">"The Intuitive Appeal of Explainable Machines"</a> (2018)</li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;David Weinberger, <a href="https://medium.com/berkman-klein-center/optimization-over-explanation-41ecb135763d">"Optimization over Explanation"</a> (2018), medium</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Finale Doshi-Velez, Mason Kortz, <a href="https://arxiv.org/abs/1711.01134">"Accountability of AI Under the Law: The Role of Explanation"</a>, arXiv preprint 1711.01134</a></li>
            <li><span class="icon-book"></span>&nbsp;&nbsp;&nbsp;Bryce Goodman, Seth Flaxman, <a href="https://arxiv.org/abs/1606.08813">"EU regulations on algorithmic decision-making and a "right to explanation""</a>, arXiv preprint 1606.08813</a></li>
          </ul>

          <h3>Friday, June 1, 2018</h3>
          <ul>
            <li>No class, but attend the <a href="https://show.mat.ucsb.edu/">2018 End of Year Show</a></li>
          </ul>

          <h3>Wednesday, June 6, 2018</h3>
          <ul>
            <li>Wrap-up discussion</li>
          </ul>

          <h3>Friday, June 8, 2018</h3>
          <ul>
            <li>Final project presentations</li>
          </ul>

          <h2>Further Resources</h2>
          <ul>
            <li><a href="https://ml4a.github.io/ml4a/">Machine Learning for Artists</a></li>
            <li><a href="https://www.youtube.com/watch?v=aircAruvnKk">"But What Is a Neural Network?" (2017), parts 1-3</a></li>
            <li><a href="https://medium.com/artists-and-machine-intelligence">Artists and Machine Intelligence</a></li>
            <li><a href="http://rhizome.org/editorial/2013/feb/19/queer-computing-1/">A Queer History of Computing</a></li>
            <li><a href="https://github.com/oxford-cs-deepnlp-2017/lectures"><span class="menu icon-github"></span>&nbsp;&nbsp;oxford-cs-deepnlp-2017/lectures</a></li>
            <li><a href="https://github.com/zhangqianhui/AdversarialNetsPapers"><span class="menu icon-github"></span>&nbsp;&nbsp;zhangqianhui/AdversarialNetsPapers</a></li>
            <li><a href="https://socialmediacollective.org/reading-lists/critical-algorithm-studies/">Critical Algorithm Studies reading list</a></li>
            <li><a href="https://arxiv.org/html/1611.09139v1">Proceedings of NIPS 2016 Workshop on Interpretable Machine Learning for Complex Systems</a></li>
            <li><a href="https://arxiv.org/html/1711.09889">Proceedings of NIPS 2017 Symposium on Interpretable Machine Learning</a></li>
            <li><a href="http://proceedings.mlr.press/v81/">Proceedings of Machine Learning Research 81: Conference on Fairness, Accountability and Transparency, 23-24 February 2018, New York, NY, USA</a></li>
            <li><a href="https://link.springer.com/journal/13347/30/3/page/1">Philosophy and Technology 30, no. 3 (2017): Rethinking Art and Aesthetics in the Age of Creative Machines</a></li>
            <li><a href="https://medium.com/@eirinimalliaraki/toward-ethical-transparent-and-fair-ai-ml-a-critical-reading-list-d950e70a70ea">Toward ethical, transparent and fair AI/ML: a critical reading list (2018), Medium</a></li>
            <li><a href="https://speak-statistics-to-power.github.io/fairness/">Mirror Mirror. Reflections on Quantitative Fairness (2018)</a></li>
            <li><a href="https://issuu.com/digicultlibrary/docs/digimag76/1?ff=true">DIGIMAG 76: Smart Machines for Enhanced Arts (2017)</a></li>
            <li><a href="https://developers.google.com/machine-learning/crash-course/">Google Machine Learning Crash Course (2018)</a></li>
            <li><a href="https://web.stanford.edu/class/cs221/">Stanford CS221: Artificial Intelligence: Principles and Techniques</a></li>
            <li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-080-great-ideas-in-theoretical-computer-science-spring-2008/index.htm">MIT 6.080/6.089: Great Ideas in Theoretical Computer Science (taught by Scott Aaronsen)</a></li>
          </ul>